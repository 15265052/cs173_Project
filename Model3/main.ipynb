{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from os.path import exists\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 7\n",
    "seq_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "global input_size\n",
    "input_size = 300\n",
    "train_test_threshold = {'sample.json':(30,60),'classified_data_0.json':(100,425), \\\n",
    "\t'classified_data_1.json':(100,440), 'classified_data_2.json':(60,150)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(filename: str):\n",
    "\tglobal input_size\n",
    "\twith open(filename) as f:\n",
    "\t\tdata = json.load(f)\n",
    "\tdel data['上海市']\n",
    "\t\n",
    "\ttotal = 0\n",
    "\tuseful = 0\n",
    "\ttrain_city = set()\n",
    "\ttest_city = set()\n",
    "\tfor city in data.values():\n",
    "\t\ttotal += len(city)\n",
    "\t\tif len(city) > train_test_threshold[filename][1]:\n",
    "\t\t\ttest_city.add(city[0]['true_city'])\n",
    "\t\t\ttrain_city.add(city[0]['true_city'])\n",
    "\t\telif len(city) > train_test_threshold[filename][0]:\n",
    "\t\t\ttrain_city.add(city[0]['true_city'])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\tuseful += 1\t\n",
    "\n",
    "\ttemp = np.empty((useful,input_size))\n",
    "\ttemp_i = 0\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\ttemp[temp_i] = p['patient_vector']\n",
    "\t\t\t\ttemp_i += 1\n",
    "\n",
    "\tpca = PCA(n_components=8)\n",
    "\tpca_result = pca.fit_transform(temp)\n",
    "\n",
    "\ttemp_i = 0\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\tp['patient_vector'] = pca_result[temp_i]\n",
    "\t\t\t\ttemp_i += 1\n",
    "\n",
    "\tinput_size = pca_result.shape[1]\n",
    "\tdatas = {}\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tstart_date = datetime.strptime(city[0]['time'],'%Y:%m:%d')\n",
    "\t\tend_date = datetime.strptime(city[-1]['time'],'%Y:%m:%d')\n",
    "\t\tperiod = (end_date-start_date).days+1\n",
    "\t\tCNT = np.zeros((period))\n",
    "\t\tVECTOR = np.zeros((period, input_size))\n",
    "\t\tcnt = 0\n",
    "\t\tvector = []\n",
    "\t\tday_idx = 0\n",
    "\t\tfor p in city:\n",
    "\t\t\tif p['time'] != (start_date).strftime('%Y:%m:%d'):\n",
    "\t\t\t\tif any((type(v)==np.ndarray for v in vector)):\n",
    "\t\t\t\t\tvector = np.array([v for v in vector if type(v)==np.ndarray])\n",
    "\t\t\t\t\tVECTOR[day_idx] = np.mean(vector, axis=0)\n",
    "\t\t\t\tCNT[day_idx] = cnt\n",
    "\t\t\t\tcnt = 0\n",
    "\t\t\t\tvector = []\n",
    "\t\t\twhile p['time'] != (start_date).strftime('%Y:%m:%d'):\n",
    "\t\t\t\tstart_date += timedelta(days=1)\n",
    "\t\t\t\tday_idx += 1\n",
    "\t\t\tcnt += 1\n",
    "\t\t\tvector.append(p['patient_vector'])\n",
    "\t\telse:\n",
    "\t\t\tif any((type(v)==np.ndarray for v in vector)):\n",
    "\t\t\t\tvector = np.array([v for v in vector if type(v)==np.ndarray])\n",
    "\t\t\t\tVECTOR[day_idx] = np.mean(vector,axis=0)\n",
    "\t\t\tCNT[day_idx] = cnt\n",
    "\t\ttemp = np.hstack((CNT.reshape(-1,1),VECTOR))\n",
    "\t\tdatas[city_string] = temp\t\n",
    "\n",
    "\treturn datas, train_city, test_city\n",
    "\n",
    "datas, train_city, test_city = feature_engineering('classified_data_0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_toomany_0(rdata:np.ndarray, seq_len:int):\n",
    "\tmove0 = []\n",
    "\tfor i in range(len(rdata)-seq_len):\n",
    "\t\tif any(rdata[i:i+seq_len,0]):\n",
    "\t\t\tmove0.append(1)\n",
    "\t\telse:\n",
    "\t\t\tmove0.append(0)\n",
    "\ttemp_i = 0\n",
    "\tsize = seq_len+1\n",
    "\tfor i in range(1,len(move0)):\n",
    "\t\tif move0[i-1] or move0[i]:\n",
    "\t\t\tsize += 1\n",
    "\tnew_data = np.zeros((size,len(rdata[0])))\n",
    "\tnew_data[0] = rdata[0]\n",
    "\tfor i in range(1,len(move0)):\n",
    "\t\tif move0[i-1] or move0[i]:\n",
    "\t\t\ttemp_i += 1\n",
    "\t\t\tnew_data[temp_i] = rdata[i]\n",
    "\tnew_data[-seq_len:]\t= rdata[-seq_len:]\n",
    "\treturn new_data\n",
    "\n",
    "for city in datas:\n",
    "\ttemp = datas[city]\n",
    "\ttemp = remove_toomany_0(datas[city],seq_len)\n",
    "\tdatas[city] = torch.tensor(temp.reshape(-1,1,input_size+1)).float()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_concat_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(Model_concat_gru, self).__init__()\n",
    "        # concat\n",
    "        self.gru = nn.GRU(input_size=input_size+1, \\\n",
    "            hidden_size=hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = output.squeeze()\n",
    "        output = self.linear(output[-1]).squeeze()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_concat_deep_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(Model_concat_deep_gru, self).__init__()\n",
    "        # concat\n",
    "        self.gru = nn.GRU(input_size=input_size+1, \\\n",
    "            hidden_size=hidden_size,num_layers=2,dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = output.squeeze()\n",
    "        output = self.linear(output[-1]).squeeze()\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_add_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size_1,hidden_size_2):\n",
    "        super(Model_add_gru, self).__init__()\n",
    "        # word embedding\n",
    "        self.gru1 = nn.GRU(input_size=input_size, \\\n",
    "            hidden_size=hidden_size_1)\n",
    "        # number\n",
    "        self.gru2 = nn.GRU(input_size=1, \\\n",
    "            hidden_size=hidden_size_2)\n",
    "        self.linear1= nn.Linear(hidden_size_1,1)\n",
    "        self.linear2 = nn.Linear(hidden_size_2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _ = self.gru1(x[:,:,1:])\n",
    "        x2, _ = self.gru2(x[:,:,0:1])\n",
    "        x1 = self.linear1(x1[-1]).squeeze()\n",
    "        x2 = self.linear2(x2[-1]).squeeze()\n",
    "        return x1+x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_add_deep_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size_1,hidden_size_2):\n",
    "        super(Model_add_deep_gru, self).__init__()\n",
    "        # word embedding\n",
    "        self.gru1 = nn.GRU(input_size=input_size, \\\n",
    "            hidden_size=hidden_size_1,num_layers=2,dropout=0.2)\n",
    "        # number\n",
    "        self.gru2 = nn.GRU(input_size=1, \\\n",
    "            hidden_size=hidden_size_2,num_layers=2,dropout=0.2)\n",
    "        self.linear1= nn.Linear(hidden_size_1,1)\n",
    "        self.linear2 = nn.Linear(hidden_size_2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _ = self.gru1(x[:,:,1:])\n",
    "        x2, _ = self.gru2(x[:,:,0:1])\n",
    "        x1 = self.linear1(x1[-1]).squeeze()\n",
    "        x2 = self.linear2(x2[-1]).squeeze()\n",
    "        return x1+x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(data,seq_len,test_size,in_test):\n",
    "\tif in_test:\n",
    "\t\tindexs = np.arange(len(data)-seq_len-test_size)\n",
    "\telse:\n",
    "\t\tindexs = np.arange(len(data)-seq_len)\n",
    "\trandom.shuffle(indexs)\n",
    "\tfor i in indexs:\n",
    "\t\tyield data[i:i+seq_len],data[i+seq_len][0][0]\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datas, test_size):\n",
    "    # create your optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epoches = 100\n",
    "    seq_len = 7\n",
    "    for epoch in tqdm(range(epoches)):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        city_keys = list(datas.keys())\n",
    "        random.shuffle(city_keys)\n",
    "        for city in city_keys:\n",
    "            \n",
    "            for data in data_iter(datas[city],seq_len,test_size,city in test_city):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        if epoch%10 == 9:\n",
    "            print(f'{epoch+1}: {running_loss:.2f}')\n",
    "\n",
    "        running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datas, test_size):\n",
    "\ttotal_loss = 0.0\n",
    "\tfor i in range(test_size, 0, -1):\n",
    "\t\tdaily_loss = 0.0\n",
    "\t\tfor city_string in test_city:\n",
    "\t\t\tinputs = datas[city_string][-i-seq_len:-i]\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\ty = float(outputs.relu())\n",
    "\t\t\ty_pred = [round(y)]\n",
    "\t\t\ty_true = [int(datas[city_string][-i][0][0])]\n",
    "\t\t\tprint(city_string, f'{y:.3f}', y_pred[0], y_true[0])\n",
    "\t\t\tdaily_loss += mean_squared_error(y_true, y_pred)\n",
    "\t\tprint(daily_loss)\n",
    "\t\ttotal_loss += daily_loss\n",
    "\tprint(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 'sample.json'\n",
    "# hidden_size = (4, 8, 12)\n",
    "# test_size = 2\n",
    "# for hz in hidden_size:\n",
    "# \tmodel = Model_concat_gru(input_size, hz)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# for hz in hidden_size:\n",
    "# \tmodel = Model_concat_deep_gru(input_size, hz)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# hidden_size = ((2,4), (2,8), (4,4), (4,8))\n",
    "# for hz1,hz2 in hidden_size:\n",
    "# \tmodel = Model_add_gru(input_size, hz1, hz2)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# for hz1,hz2 in hidden_size:\n",
    "# \tmodel = Model_add_deep_gru(input_size, hz1, hz2)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "石家庄市 0.329 0 0\n",
      "北京市 21.487 21 38\n",
      "西安市 0.514 1 0\n",
      "呼和浩特市 1.476 1 6\n",
      "郑州市 1.380 1 18\n",
      "604.0\n",
      "石家庄市 0.285 0 0\n",
      "北京市 33.268 33 10\n",
      "西安市 0.815 1 10\n",
      "呼和浩特市 3.253 3 5\n",
      "郑州市 20.633 21 44\n",
      "1143.0\n",
      "石家庄市 0.725 1 0\n",
      "北京市 15.607 16 34\n",
      "西安市 7.926 8 0\n",
      "呼和浩特市 5.432 5 1\n",
      "郑州市 26.085 26 56\n",
      "1305.0\n",
      "石家庄市 0.260 0 0\n",
      "北京市 42.604 43 14\n",
      "西安市 12.358 12 6\n",
      "呼和浩特市 0.654 1 4\n",
      "郑州市 27.503 28 36\n",
      "950.0\n",
      "石家庄市 0.074 0 0\n",
      "北京市 21.431 21 14\n",
      "西安市 5.610 6 1\n",
      "呼和浩特市 3.394 3 1\n",
      "郑州市 20.796 21 47\n",
      "754.0\n",
      "石家庄市 0.373 0 0\n",
      "北京市 10.929 11 24\n",
      "西安市 1.186 1 11\n",
      "呼和浩特市 0.242 0 1\n",
      "郑州市 32.067 32 38\n",
      "306.0\n",
      "石家庄市 2.151 2 1\n",
      "北京市 19.646 20 14\n",
      "西安市 9.772 10 3\n",
      "呼和浩特市 2.357 2 1\n",
      "郑州市 36.429 36 17\n",
      "448.0\n",
      "5510.0\n"
     ]
    }
   ],
   "source": [
    "test_size = 7\n",
    "model1 = Model_concat_gru(input_size, 8)\n",
    "train(model1, datas, test_size)\n",
    "test(model1, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:06<10:12,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 89053.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:11<09:16,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: 86591.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [03:14<07:12,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: 79440.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:19<06:39,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: 80217.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [05:24<05:23,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: 75721.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [06:32<04:38,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60: 76709.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [07:40<03:18,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70: 72639.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [08:52<02:28,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80: 75189.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [10:04<01:08,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90: 73308.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:15<00:00,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 75347.45\n",
      "Finished Training\n",
      "石家庄市 2.813 3 0\n",
      "北京市 15.364 15 38\n",
      "西安市 1.297 1 0\n",
      "呼和浩特市 2.622 3 6\n",
      "郑州市 3.128 3 18\n",
      "773.0\n",
      "石家庄市 1.161 1 0\n",
      "北京市 32.921 33 10\n",
      "西安市 0.613 1 10\n",
      "呼和浩特市 3.681 4 5\n",
      "郑州市 17.944 18 44\n",
      "1288.0\n",
      "石家庄市 1.005 1 0\n",
      "北京市 18.859 19 34\n",
      "西安市 4.849 5 0\n",
      "呼和浩特市 4.064 4 1\n",
      "郑州市 30.049 30 56\n",
      "936.0\n",
      "石家庄市 0.558 1 0\n",
      "北京市 29.044 29 14\n",
      "西安市 7.353 7 6\n",
      "呼和浩特市 1.905 2 4\n",
      "郑州市 30.575 31 36\n",
      "256.0\n",
      "石家庄市 0.653 1 0\n",
      "北京市 13.081 13 14\n",
      "西安市 7.853 8 1\n",
      "呼和浩特市 7.633 8 1\n",
      "郑州市 18.632 19 47\n",
      "884.0\n",
      "石家庄市 0.605 1 0\n",
      "北京市 15.188 15 24\n",
      "西安市 0.909 1 11\n",
      "呼和浩特市 1.767 2 1\n",
      "郑州市 31.282 31 38\n",
      "232.0\n",
      "石家庄市 1.080 1 1\n",
      "北京市 13.696 14 14\n",
      "西安市 6.608 7 3\n",
      "呼和浩特市 2.140 2 1\n",
      "郑州市 32.560 33 17\n",
      "273.0\n",
      "4642.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Model_concat_deep_gru(input_size, 4)\n",
    "train(model2, datas, test_size)\n",
    "test(model2, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:00<09:15,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 83326.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:07<09:03,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: 77912.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [03:06<06:27,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: 75227.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:01<05:47,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: 75150.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [05:05<05:31,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: 70768.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [06:13<04:18,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60: 70076.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [07:17<03:08,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70: 68789.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [08:20<02:03,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80: 67849.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [09:22<01:02,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90: 66481.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:30<00:00,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 65993.05\n",
      "Finished Training\n",
      "石家庄市 1.267 1 0\n",
      "北京市 17.828 18 38\n",
      "西安市 0.507 1 0\n",
      "呼和浩特市 5.336 5 6\n",
      "郑州市 4.611 5 18\n",
      "572.0\n",
      "石家庄市 1.011 1 0\n",
      "北京市 25.435 25 10\n",
      "西安市 0.668 1 10\n",
      "呼和浩特市 6.078 6 5\n",
      "郑州市 16.975 17 44\n",
      "1037.0\n",
      "石家庄市 1.156 1 0\n",
      "北京市 10.071 10 34\n",
      "西安市 6.849 7 0\n",
      "呼和浩特市 5.329 5 1\n",
      "郑州市 27.060 27 56\n",
      "1483.0\n",
      "石家庄市 0.823 1 0\n",
      "北京市 27.810 28 14\n",
      "西安市 4.746 5 6\n",
      "呼和浩特市 0.974 1 4\n",
      "郑州市 30.493 30 36\n",
      "243.0\n",
      "石家庄市 0.542 1 0\n",
      "北京市 16.708 17 14\n",
      "西安市 6.404 6 1\n",
      "呼和浩特市 4.129 4 1\n",
      "郑州市 32.633 33 47\n",
      "240.0\n",
      "石家庄市 0.576 1 0\n",
      "北京市 11.885 12 24\n",
      "西安市 3.112 3 11\n",
      "呼和浩特市 2.442 2 1\n",
      "郑州市 45.931 46 38\n",
      "274.0\n",
      "石家庄市 1.154 1 1\n",
      "北京市 15.862 16 14\n",
      "西安市 9.031 9 3\n",
      "呼和浩特市 2.213 2 1\n",
      "郑州市 37.340 37 17\n",
      "441.0\n",
      "4290.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = Model_add_gru(input_size, 2, 4)\n",
    "train(model3, datas, test_size)\n",
    "test(model3, datas, test_size)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfa0342052044840b0de6043bfc39f6805ffa1411cba6a71c8075fbda85253ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
