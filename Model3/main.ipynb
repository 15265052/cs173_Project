{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from os.path import exists\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 7\n",
    "seq_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global input_size\n",
    "input_size = 300\n",
    "train_test_threshold = {'sample.json':(30,60),'classified_data_0.json':(100,425), \\\n",
    "\t'classified_data_1.json':(100,440), 'classified_data_2.json':(60,150)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(filename: str):\n",
    "\tglobal input_size\n",
    "\twith open(filename) as f:\n",
    "\t\tdata = json.load(f)\n",
    "\tdel data['上海市']\n",
    "\t\n",
    "\ttotal = 0\n",
    "\tuseful = 0\n",
    "\ttrain_city = set()\n",
    "\ttest_city = set()\n",
    "\tfor city in data.values():\n",
    "\t\ttotal += len(city)\n",
    "\t\tif len(city) > train_test_threshold[filename][1]:\n",
    "\t\t\ttest_city.add(city[0]['true_city'])\n",
    "\t\t\ttrain_city.add(city[0]['true_city'])\n",
    "\t\telif len(city) > train_test_threshold[filename][0]:\n",
    "\t\t\ttrain_city.add(city[0]['true_city'])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\tuseful += 1\t\n",
    "\n",
    "\ttemp = np.empty((useful,input_size))\n",
    "\ttemp_i = 0\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\ttemp[temp_i] = p['patient_vector']\n",
    "\t\t\t\ttemp_i += 1\n",
    "\n",
    "\tpca = PCA(n_components=8)\n",
    "\tpca_result = pca.fit_transform(temp)\n",
    "\n",
    "\ttemp_i = 0\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tfor p in city:\n",
    "\t\t\tif type(p['patient_vector']) == list:\n",
    "\t\t\t\tp['patient_vector'] = pca_result[temp_i]\n",
    "\t\t\t\ttemp_i += 1\n",
    "\n",
    "\tinput_size = pca_result.shape[1]\n",
    "\tdatas = {}\n",
    "\tfor city_string in train_city:\n",
    "\t\tcity = data[city_string]\n",
    "\t\tstart_date = datetime.strptime(city[0]['time'],'%Y:%m:%d')\n",
    "\t\tend_date = datetime.strptime(city[-1]['time'],'%Y:%m:%d')\n",
    "\t\tperiod = (end_date-start_date).days+1\n",
    "\t\tCNT = np.zeros((period))\n",
    "\t\tVECTOR = np.zeros((period, input_size))\n",
    "\t\tcnt = 0\n",
    "\t\tvector = []\n",
    "\t\tday_idx = 0\n",
    "\t\tfor p in city:\n",
    "\t\t\tif p['time'] != (start_date).strftime('%Y:%m:%d'):\n",
    "\t\t\t\tif any((type(v)==np.ndarray for v in vector)):\n",
    "\t\t\t\t\tvector = np.array([v for v in vector if type(v)==np.ndarray])\n",
    "\t\t\t\t\tVECTOR[day_idx] = np.mean(vector, axis=0)\n",
    "\t\t\t\tCNT[day_idx] = cnt\n",
    "\t\t\t\tcnt = 0\n",
    "\t\t\t\tvector = []\n",
    "\t\t\twhile p['time'] != (start_date).strftime('%Y:%m:%d'):\n",
    "\t\t\t\tstart_date += timedelta(days=1)\n",
    "\t\t\t\tday_idx += 1\n",
    "\t\t\tcnt += 1\n",
    "\t\t\tvector.append(p['patient_vector'])\n",
    "\t\telse:\n",
    "\t\t\tif any((type(v)==np.ndarray for v in vector)):\n",
    "\t\t\t\tvector = np.array([v for v in vector if type(v)==np.ndarray])\n",
    "\t\t\t\tVECTOR[day_idx] = np.mean(vector,axis=0)\n",
    "\t\t\tCNT[day_idx] = cnt\n",
    "\t\ttemp = np.hstack((CNT.reshape(-1,1),VECTOR))\n",
    "\t\tdatas[city_string] = temp\t\n",
    "\n",
    "\treturn datas, train_city, test_city\n",
    "\n",
    "datas, train_city, test_city = feature_engineering('classified_data_0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_toomany_0(rdata:np.ndarray, seq_len:int):\n",
    "\tmove0 = []\n",
    "\tfor i in range(len(rdata)-seq_len):\n",
    "\t\tif any(rdata[i:i+seq_len,0]):\n",
    "\t\t\tmove0.append(1)\n",
    "\t\telse:\n",
    "\t\t\tmove0.append(0)\n",
    "\ttemp_i = 0\n",
    "\tsize = seq_len+1\n",
    "\tfor i in range(1,len(move0)):\n",
    "\t\tif move0[i-1] or move0[i]:\n",
    "\t\t\tsize += 1\n",
    "\tnew_data = np.zeros((size,len(rdata[0])))\n",
    "\tnew_data[0] = rdata[0]\n",
    "\tfor i in range(1,len(move0)):\n",
    "\t\tif move0[i-1] or move0[i]:\n",
    "\t\t\ttemp_i += 1\n",
    "\t\t\tnew_data[temp_i] = rdata[i]\n",
    "\tnew_data[-seq_len:]\t= rdata[-seq_len:]\n",
    "\treturn new_data\n",
    "\n",
    "for city in datas:\n",
    "\ttemp = datas[city]\n",
    "\ttemp = remove_toomany_0(datas[city],seq_len)\n",
    "\tdatas[city] = torch.tensor(temp.reshape(-1,1,input_size+1)).float()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_concat_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(Model_concat_gru, self).__init__()\n",
    "        # concat\n",
    "        self.gru = nn.GRU(input_size=input_size+1, \\\n",
    "            hidden_size=hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = output.squeeze()\n",
    "        output = self.linear(output[-1]).squeeze()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_concat_deep_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(Model_concat_deep_gru, self).__init__()\n",
    "        # concat\n",
    "        self.gru = nn.GRU(input_size=input_size+1, \\\n",
    "            hidden_size=hidden_size,num_layers=2,dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = output.squeeze()\n",
    "        output = self.linear(output[-1]).squeeze()\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_add_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size_1,hidden_size_2):\n",
    "        super(Model_add_gru, self).__init__()\n",
    "        # word embedding\n",
    "        self.gru1 = nn.GRU(input_size=input_size, \\\n",
    "            hidden_size=hidden_size_1)\n",
    "        # number\n",
    "        self.gru2 = nn.GRU(input_size=1, \\\n",
    "            hidden_size=hidden_size_2)\n",
    "        self.linear1= nn.Linear(hidden_size_1,1)\n",
    "        self.linear2 = nn.Linear(hidden_size_2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _ = self.gru1(x[:,:,1:])\n",
    "        x2, _ = self.gru2(x[:,:,0:1])\n",
    "        x1 = self.linear1(x1[-1]).squeeze()\n",
    "        x2 = self.linear2(x2[-1]).squeeze()\n",
    "        return x1+x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_add_deep_gru(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size_1,hidden_size_2):\n",
    "        super(Model_add_deep_gru, self).__init__()\n",
    "        # word embedding\n",
    "        self.gru1 = nn.GRU(input_size=input_size, \\\n",
    "            hidden_size=hidden_size_1,num_layers=2,dropout=0.2)\n",
    "        # number\n",
    "        self.gru2 = nn.GRU(input_size=1, \\\n",
    "            hidden_size=hidden_size_2,num_layers=2,dropout=0.2)\n",
    "        self.linear1= nn.Linear(hidden_size_1,1)\n",
    "        self.linear2 = nn.Linear(hidden_size_2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _ = self.gru1(x[:,:,1:])\n",
    "        x2, _ = self.gru2(x[:,:,0:1])\n",
    "        x1 = self.linear1(x1[-1]).squeeze()\n",
    "        x2 = self.linear2(x2[-1]).squeeze()\n",
    "        return x1+x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(data,seq_len,test_size,in_test):\n",
    "\tif in_test:\n",
    "\t\tindexs = np.arange(len(data)-seq_len-test_size)\n",
    "\telse:\n",
    "\t\tindexs = np.arange(len(data)-seq_len)\n",
    "\trandom.shuffle(indexs)\n",
    "\tfor i in indexs:\n",
    "\t\tyield data[i:i+seq_len],data[i+seq_len][0][0]\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datas, test_size):\n",
    "    # create your optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epoches = 100\n",
    "    seq_len = 7\n",
    "    for epoch in tqdm(range(epoches)):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        city_keys = list(datas.keys())\n",
    "        random.shuffle(city_keys)\n",
    "        for city in city_keys:\n",
    "            \n",
    "            for data in data_iter(datas[city],seq_len,test_size,city in test_city):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        if epoch%10 == 9:\n",
    "            print(f'{epoch+1}: {running_loss:.2f}')\n",
    "\n",
    "        running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datas, test_size):\n",
    "\ttotal_loss = 0.0\n",
    "\tfor i in range(test_size, 0, -1):\n",
    "\t\tdaily_loss = 0.0\n",
    "\t\tfor city_string in test_city:\n",
    "\t\t\tinputs = datas[city_string][-i-seq_len:-i]\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\ty = float(outputs.relu())\n",
    "\t\t\ty_pred = [round(y)]\n",
    "\t\t\ty_true = [int(datas[city_string][-i][0][0])]\n",
    "\t\t\tprint(city_string, f'{y:.3f}', y_pred[0], y_true[0])\n",
    "\t\t\tdaily_loss += mean_squared_error(y_true, y_pred)\n",
    "\t\tprint(daily_loss)\n",
    "\t\ttotal_loss += daily_loss\n",
    "\tprint(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 'sample.json'\n",
    "# hidden_size = (4, 8, 12)\n",
    "# test_size = 2\n",
    "# for hz in hidden_size:\n",
    "# \tmodel = Model_concat_gru(input_size, hz)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# for hz in hidden_size:\n",
    "# \tmodel = Model_concat_deep_gru(input_size, hz)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# hidden_size = ((2,4), (2,8), (4,4), (4,8))\n",
    "# for hz1,hz2 in hidden_size:\n",
    "# \tmodel = Model_add_gru(input_size, hz1, hz2)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n",
    "# for hz1,hz2 in hidden_size:\n",
    "# \tmodel = Model_add_deep_gru(input_size, hz1, hz2)\n",
    "# \ttrain(model, datas, test_size)\n",
    "# \ttest(model, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 7\n",
    "model1 = Model_concat_gru(input_size, 8)\n",
    "train(model1, datas, test_size)\n",
    "test(model1, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model_concat_deep_gru(input_size, 4)\n",
    "train(model2, datas, test_size)\n",
    "test(model2, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model_add_gru(input_size, 2, 4)\n",
    "train(model3, datas, test_size)\n",
    "test(model3, datas, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mse = [[5510.0, 4642.0, 3680.0], [5122.0, 4547.0, 4618.0], [461.0, 288.0, 240.0]]\n",
    "data_rmse = [[np.sqrt(data_mse[i][j]/(5*7)) for i in range(3)] for j in range(3)]\n",
    "data_rmse = np.array(data_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "x = np.arange(3)\n",
    "total_width, n = 0.8, 3\n",
    "width = total_width/n\n",
    "x = x - (total_width - width)/2\n",
    "labels = ['data_0', 'data_1', 'data_2']\n",
    "plt.bar(x, data_rmse[0], width, label='model_1')\n",
    "plt.bar(x+width, data_rmse[1], width, label='model_2')\n",
    "plt.bar(x+2*width, data_rmse[2], width, label='model_3')\n",
    "plt.title('Preformance Evaluation By RMSE')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.xticks(np.arange(3), labels=labels)\n",
    "plt.legend()\n",
    "plt.savefig('result')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfa0342052044840b0de6043bfc39f6805ffa1411cba6a71c8075fbda85253ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
